% DO NOT EDIT - automatically generated from metadata-draft.yaml

\def \codeURL{https://github.com/dido1998/h-detach}
\def \codeDOI{}
\def \dataURL{}
\def \dataDOI{}
\def \editorNAME{}
\def \editorORCID{}
\def \reviewerINAME{}
\def \reviewerIORCID{}
\def \reviewerIINAME{}
\def \reviewerIIORCID{}
\def \dateRECEIVED{01 November 2018}
\def \dateACCEPTED{}
\def \datePUBLISHED{}
\def \articleTITLE{h-detach : Modifying the LSTM gradient towards better optimization ICLR Reproducibility Challenge }
\def \articleTYPE{Editorial}
\def \articleDOMAIN{}
\def \articleBIBLIOGRAPHY{bibliography.bib}
\def \articleYEAR{2019}
\def \reviewURL{https://github.com/reproducibility-challenge/iclr_2019/pull/148}
\def \articleABSTRACT{Recurrent neural networks have been widely used for processing sequences. Recurrent neural networks are known for their exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve them exist over long time scales, because
EVGP prevents important gradient components from being back-propagated adequately over a large number of steps. This paper aims to reproduce the results of the paper \cite{kanuparthi2018hdetach}. \cite{kanuparthi2018hdetach}  introduces a stochastic algorithm (h-detach) to mitigate the EVGP problem in Long Short Term Memory networks. Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.}
\def \replicationCITE{}
\def \replicationBIB{}
\def \replicationURL{}
\def \replicationDOI{}
\def \contactNAME{Aniket Didolkar}
\def \contactEMAIL{adidolkar123@gmail.com}
\def \articleKEYWORDS{rescience c, rescience x}
\def \journalNAME{ReScience C}
\def \journalVOLUME{4}
\def \journalISSUE{1}
\def \articleNUMBER{}
\def \articleDOI{}
\def \authorsFULL{Aniket Didolkar}
\def \authorsABBRV{ Aniket Didolkar}
\def \authorsSHORT{Aniket }
\title{\articleTITLE}
\date{}
\author[1\orcid{ 0000-0001-9183-3144}]{Aniket Didolkar}

\affil[1]{Manipal Academy of Higher Education, Manipal, India}

